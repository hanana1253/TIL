# Django 

## 네이버 영화평점 Web Scraping
- 지난 시간에 이어, 웹 브라우저가 어떻게 서버에 요청을 하고 응답을 받는지 스크래핑을 통해 알아보자.
- 현재 상영중인 영화에 대해 각각 별점과 댓글 데이터를 가져온다.

### 접근 순서
- 각 영화는 제목과는 별개로 자신을 식별할 수 있는 키를 가지고 있을 것이다.
  - 웹화면을 구성하는 3요소: UI, 데이터, 로직(어떻게 화면에 보여줄지)
  - 각각의 영화 페이지는 다른 마크업을 렌더링하는 게 아니라 같은 페이지 틀에서 데이터만 바뀌는 것이다.
  - 이 데이터를 지칭하는 유니크한 키(primary key)가 따로 있어야 한다. (제목은 동명이인과 같이 혼돈의 소지가 있음)
- 각 영화와 그 영화를 가리키는 키의 목록을 데이터로 만든다. 그 후 그 안에 댓글데이터를 추가한다.
- 제목과 키는 URL만 있으면 충분히 가져올 수 있다. 하지만 이렇게 쉽게 가져올 수 있는 데이터는 많지 않다.
- DOM 트리 생성 후에 오는 데이터는 서버에서 오는 것이기 때문에 URL에서 가져오기가 힘들다. 
  - 웹브라우저가 서버에 요청하는 것을 똑같이 보내서 받아올 수 있는데 이를 API 스크래핑이라고 부르자.
  - 서버의 API는 특정 요청이 오면 해당하는 응답을 주도록 API가 만들어져 있기 대문에 우리가 그 요청을 따라하면 같은 데이터를 가져올 수 있다. 
  - 브라우저에서 가져오는 게 아니라 서버에서 바로 가져온다.

### 개발자도구를 통한 HTTP 이해하기
- HTTP: 웹이 동작하는 방식에 대한 규칙
  - 모바일 앱 중에서도 웹에 기반하여 동작하게 하고 모바일로만 접근하게 제한을 둔 경우가 있는데, 데스크탑으로도 user-agent를 모바일로 바꾸면 접속이 가능하다. 
- 개발자도구에서 HTTP을 다루는 것에 친숙해지자.

#### Headers General 
전체 요청과 응답의 요약, 어디로 요청을 보냈고 어떤 응답이 왔는지(상태 코드) 알 수 있다.
- Request URL: 도메인 주소
- Remote Address: 실제 서버 주소(IP)와 포트 (`:` 뒤에 오는 게 포트번호로, 443은 보안된 https, 80은 보안이 안되는 http)
- Request Method: HTTP request method (GET, POST, PUT, DELETE 등)
- Referrer Policy: 방문출처정책, 이 사이트에 접속하기 전 어디로부터 왔는지에 대해 대응한다. no-referrer은 신경쓰지 않겠다는 정책이라 header에 referrer을 굳이 포함시키지 않는다. 어느 광고에서 유입이 많이 되었는지 확인할 수 있는 등 유입정보에 따른 전략을 세우기에 유용하다. 

#### Response Headers
- Accept: 어떤 식으로 응답을 보내달라는 것인지 형식이 명시되어 있으며, *는 어떤 형태든 상관없다는 뜻
- Cache-control
  - `cache`: 자주 사용하는 데이터를 저장한 임시저장소
  - 자주 들어오는 요청이나 바뀌지 않을 요청에 대해 브라우저에 cache로 저장해두고 빠르게 access하도록 최적화한다
  - 갑자기 많은 요청이 들어오는 경우(Flash Crowds)에 cache server을 잘 설계함으로써 병목현상을 예방할 수 있다.
  - HTTP 재검사로 기존 자원과 달라진 게 있을 때 업데이트하기도 한다.
  - `cookie`: 서버에 다시 요청할 필요 없이 관리해야할 정보가 있을 때 브라우저에 저장해둔다. 로그인 유지기능이나 인증 관련하여 나를 식별해야 하는 상황에 주로 쓰인다. 
  - `session`: 사용자가 접속하고 종료하기까지의 단위를 기준으로 관리. 접속하면 세션 ID가 발급되고 종료하면 세션 ID가 만료(c.f. 쿠키는 개발자가 지정한 expiration date까지 만료되지 않는다), 세션 ID를 보내면 서버에서 식별하고 이에 맞는 정보를 보내준다. 이는 직접 유저데이터가 오가는 것보다 더 안전하다.

### 요청과 응답을 활용한 웹 스크래핑 꿀팁
- url로 렌더링된 html 뿐 아니라 JS로 요청한 데이터까지 가져오는 방법
- 개발자 도구의 Network 탭에서 `cmd+f`로 데이터로 던져진 부분의 일부를 검색해보면 해당 데이터를 요청하여 받은 response를 찾을 수 있다.
- headers 탭을 보면 해당 요청이 응답받은 URL을 볼 수 있는데, 그 URL을 찾아서 마우스 우클릭 후 Copy cURL
- 복사한 cURL을 [curl trillworks](https://curl.trillworks.com/)에서 파이썬 코드로 변환한다.
- `import requests`부터 `response = requests.get(url)`까지 다 해줬으니까 이제 필요한 부분을 BeautifulSoup으로 파싱하고 찾아내면 된다.

## 느낀 점
- 웹 스크래핑은 재미있다.
- 코드를 단순하게 정리하는 법을 익히고 싶은데 아직은 내 코드는 너저분해서 아쉽다.
